1) prepare lists

rm -f data/*/*.list; for i in data/{clanky,diskuse,tvorivost}/*; do find -L $i -type f >$i.list; done

2) tokenize raw texts (+normalize)

time ( for i in data/*/*.list; do j=`echo $i|sed -e 's/[^\/]*\/\(.*\)\.list/\1/'`; hector_server -c WebCorpus.xml -fb tokenize $i tokenized/$j.hr.gz tokenized/$j.log; done )
real    246m47.748s
user    210m41.340s
sys     5m20.950s
- create data dump:
for i in tokenized/*/*.hr.gz; do j=`echo $i|sed -e 's/\.hr\.gz$/.dump/'`; hector_server -c WebCorpus.xml -fb dump $i >$j; done

3) word count for different groups:
cat tokenized/clanky/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 1742463	15558831	44761605	610849340	737026459	0	0	0	0
cat tokenized/diskuse/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 2577228	54599888	114720387	1674143968	2089012818	0	0	0	0
cat tokenized/tvorivost/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 2837309	64303734	150933542	1575023038	2037581912	0	0	0	0
cat tokenized/*/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 7157000	134462453	310415534	3860016346	4863621189	0	0	0	0

4) deduplicate (all at once), split all.hr.gz back into separate files

time ( cat tokenized/*/*.hr.gz |hector_server -c WebCorpus.xml -fb deduplicate_all 6000000000 /dev/stdin deduplicated/all.hr.gz deduplicated/deduplicate.log )
real    290m39.668s
user    289m5.890s
sys     1m8.620s
- word counts (before/after)
M_wc1[0]: 7157000	134462453	310415534	3860016346	4863621189	0	0	0	0
M_wc2[0]: 6975869	117402379	262560346	3285087887	4073350543	0	0	0	0

for i in data/*/*.list; do j=`echo $i|sed -e 's/[^\/]*\/\(.*\)\.list$/\1/'`; echo $j; done|./generate_split.pl >Split.xml
time ( hector_server -c Split.xml -fb split )
real    74m29.675s
user    107m40.950s
sys     0m42.720s

cat deduplicated/clanky/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 1663586	13574159	38719760	530929131	633949478	0	0	0	0
cat deduplicated/diskuse/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 2566033	51597217	107854221	1579140642	1943496770	0	0	0	0
cat deduplicated/tvorivost/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 2746250	52231003	115986365	1175018114	1495904295	0	0	0	0
cat deduplicated/*/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 6975869	117402379	262560346	3285087887	4073350543	0	0	0	0

5) detect paragraphs with more unaccented words

time ( for i in deduplicated/*/*.hr.gz; do j=`echo $i|sed -e 's/[^\/]*\/\(.*\)\.hr\.gz$/\1/'`; hector_server -c WebCorpus.xml -fb filter_unaccented $i accented/$j.hr.gz cssk_top_words.txt accented/$j.log; done )
real    414m2.564s
user    411m59.120s
sys     0m33.950s

cat accented/clanky/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 1663358	13559528	38672583	530199011	633012695	0	0	0	0
cat accented/diskuse/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 2538384	40999834	86400645	1200157753	1478993848	0	0	0	0
cat accented/tvorivost/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 2718960	50854866	112239584	1113841653	1418577199	0	0	0	0
cat accented/*/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 6920702	105414228	237312812	2844198417	3530583742	0	0	0	0

6) language detect + filter

- document level
time ( for i in accented/clanky/*.hr.gz; do j=`echo $i|sed -e 's/[^\/]*\/\(.*\)\.hr\.gz$/\1/'`; hector_server -c WebCorpus.xml -fb filter_language $i czech/$j.hr.gz language_data/top_words cs false czech/$j.log; done )
real    46m28.904s
user    46m3.100s
sys     0m15.010s

- paragraph-level
time ( for i in accented/diskuse/*.hr.gz; do j=`echo $i|sed -e 's/[^\/]*\/\(.*\)\.hr\.gz$/\1/'`; hector_server -c WebCorpus.xml -fb filter_language $i czech/$j.hr.gz language_data/top_words cs true czech/$j.log; done )
real    123m52.773s
user    123m13.340s
sys     0m20.410s

time ( for i in accented/tvorivost/*.hr.gz; do j=`echo $i|sed -e 's/[^\/]*\/\(.*\)\.hr\.gz$/\1/'`; hector_server -c WebCorpus.xml -fb filter_language $i czech/$j.hr.gz language_data/top_words cs true czech/$j.log; done )
real    116m53.580s
user    116m7.860s
sys     0m15.620s

cat czech/clanky/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 1654708	13474344	38415495	526264047	628332859	0	0	0	0
cat czech/diskuse/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 2518913	38245529	81912045	1142799799	1407212397	0	0	0	0
cat czech/tvorivost/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 2634738	44365627	98235887	982444511	1249527108	0	0	0	0
cat czech/*/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 6808359	96085500	218563427	2651508357	3285072364	0	0	0	0

languages (in words):
clanky:
cs	496998792
sk	3068696
en	502645
other	65421
diskuse:
cs	1143025931
sk	51297131
other	4616945
en	1488891
tvorivost:
cs	982272188
sk	107607917
other	13068492
en	10963423
celkem:
cs	2622296911
sk	161973744
other	17750858
en	12954959

7) split documents into smaller files

for i in czech/*/*.hr.gz; do j=`echo $i|sed -e 's/[^\/]*\/\(.*\)/\1/'`; hector_server -c WebCorpus.xml -fb split $i split/$j 1048576; done

8) tag all files (using SGE)

for i in split/*/*.hr.gz.*; do j=`echo $i|sed -e 's/^split/tagged/'`; qsub ./tag.sh $i $j; sleep 4; done

9) parse all files

#ls tagged/*/*.hr.gz.*|grep -v '\.log$'|while read i; do j=`echo $i|sed -e 's/^tagged/parsed/'`; qsub ./parse.sh $i $j; sleep 4; done
ls tagged/*/*.hr.gz.*|grep -v '\.log$'|while read i; do j=`echo $i|sed -e 's/^tagged/parsed/'`; if [ ! -f "$j.log" ]; then qsub ./parse.sh $i $j; sleep 4; fi; done

find parsed/ -type f|grep -v ".log$"|xargs cat |hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 6808359	96085500	218563427	2651508357	3285072364	3285072364	3285072364	3285072364	3285072364

10) concatenate all files back (just plain cat should do in our case)
find parsed/*/*.log|sed -e 's/\.hr.gz\.[0-9]*\.log$//'|sort -u|while read i; do n=`echo $i.hr.gz.|wc -c`; target=`echo $i|sed -e 's/parsed\/\(.*\)/finished\/\1.hr.gz/'`; ls $i.*|grep -v '\.log'|sort -k 1.$n -n| xargs cat >$target; done

11) create three big final files (and dumps)
for f in clanky diskuse tvorivost; do cat finished/$f/*.hr.gz > finished_$f.hr.gz; done
for f in clanky diskuse tvorivost; do hector_server -c WebCorpus.xml -fb dump finished_${f}.hr.gz > finished_${f}.dump; done

12) convert files to vertical format and prepare for Manatee import
for f in finished_*.vert; do ./vert2manatee.pl <$f|./vert_add_attrs.pl time_map.txt url_map.txt |bzip2 -c >~/manatee_registry/$f.bz; done

13) create permutations

C++ program:
1) mmap data file into memory
2) get sentence boundaries (start -- end, byte offsets)
3) make permutation, write it to the file (Knuth shuffles)
4) go randomly through the file, write parts as necessary

g++ -Wall -ggdb -O3 -o shuffle_sentences shuffle_sentences.cc

for f in clanky diskuse tvorivost; do ./shuffle_sentences finished_$f.vert finished_$f.perm >finished_$f_shuffled.vert; done
gzip -dc finished_clanky.dump.shuffled.gz |./vert2export.pl |bzip2 -c >articles_shuffled.txt.bz2
gzip -dc finished_diskuse.dump.shuffled.gz |./vert2export.pl |bzip2 -c >dicussions_shuffled.txt.bz2
gzip -dc finished_tvorivost.dump.shuffled.gz |./vert2export.pl |bzip2 -c >blogs_shuffled.txt.bz2

14) create Google n-gram compatible data

1) create smaller files
for i in clanky diskuse tvorivost; do hector_server -c WebCorpus.xml -fb split finished_{$f}.hr.gz ngram/$j/finished_${j}.hr.gz 10485760; done

2) extract n-grams from every file
- pouzivat vsude: LC_ALL=cs_CZ.utf8 (!!!)
ls ngram/*/*.hr.gz.*|while read i; do qsub ./ngram_compute.sh $i ngram; sleep 4; done

3) merge ngrams
for i in 1 2 3 4 5; do for j in clanky diskuse tvorivost; do qsub ./ngram_merge.sh $j $i; sleep 4; done; done

- google has:
Total number of tokens:	1,306,807,412,486
Total number of sentences:	 150,727,365,731
Total number of unigrams:	95,998,281
Total number of bigrams:	646,439,858
Total number of trigrams:	1,312,972,925
Total number of fourgrams:	1,396,154,236
Total number of fivegrams:	1,149,361,413
Total number of n-grams:	4,600,926,713

tokens:
sentences:
...

accessoire Annuaire LOEIL	49


15) Collocations:

1) create small files (see ngram generation)

2) dump all files (convert to text representation)
FIXME: z tech mensich souboru
for i in sync/*/*.hr.gz.*; do j=`echo $i|sed -e 's/\.hr\.gz\.\(.*\)/.hr.\1/'`; hector_server -c WebCorpus.xml -fb dump $i >$j.dump; done

3) run extraction on all files
for i in sync/*/*.dump; do ./extract_syncs_pdt2.pl <$i >$i.tmp; done

4) merge all files together
ls sync/*/*.dump.tmp | ./merge_syncs.pl

5) create index (word-pos -> offset)
