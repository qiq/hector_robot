1) prepare lists

rm -f data/*/*.list; for i in data/{clanky,diskuse,tvorivost}/*; do find -L $i -type f >$i.list; done

2) tokenize raw texts (+normalize)

time ( for i in data/*/*.list; do j=`echo $i|sed -e 's/[^\/]*\/\(.*\)\.list/\1/'`; hector_server -c WebCorpus.xml -fb tokenize $i tokenized/$j.hr.gz tokenized/$j.log; done )
real    246m47.748s
user    210m41.340s
sys     5m20.950s
- create data dump:
for i in tokenized/*/*.hr.gz; do j=`echo $i|sed -e 's/\.hr\.gz$/.dump/'`; hector_server -c WebCorpus.xml -fb dump $i >$j; done

3) word count for different groups:
cat tokenized/clanky/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 1580493 14261018        41530385        568732717       686604997       0       0       0       0
cat tokenized/diskuse/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 2578840 54527889        114528831       1674143836      2083883744      0       0       0       0
cat tokenized/tvorivost/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 2831249 64050936        150339872       1574181258      2031820433      0       0       0       0
cat tokenized/*/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 6990582 132839843       306399088       3817057811      4802309174      0       0       0       0

4) deduplicate (all at once), split all.hr.gz back into separate files

time ( cat tokenized/*/*.hr.gz |hector_server -c WebCorpus.xml -fb deduplicate_all 6000000000 /dev/stdin deduplicated/all.hr.gz deduplicated/deduplicate.log )
real    290m39.668s
user    289m5.890s
sys     1m8.620s
- word counts (before/after)
M_wc1[0]: 6990582       132839843       306399088       3817057811      4802309174      0       0       0       0
M_wc2[0]: 6819394       116223396       259753763       3255271542      4030066932      0       0       0       0

for i in data/*/*.list; do j=`echo $i|sed -e 's/[^\/]*\/\(.*\)\.list$/\1/'`; echo $j; done|./generate_split.pl >Split.xml
time ( hector_server -c Split.xml -fb split )
real    74m29.675s
user    107m40.950s
sys     0m42.720s

cat deduplicated/clanky/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 1512260 12617177        36461450        501251489       598926543       0       0       0       0
cat deduplicated/diskuse/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 2566027 51551081        107697632       1579254326      1938789786      0       0       0       0
cat deduplicated/tvorivost/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 2741107 52055138        115594681       1174765727      1492350603      0       0       0       0
cat deduplicated/*/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 6819394 116223396       259753763       3255271542      4030066932      0       0       0       0

5) detect paragraphs with more unaccented words

time ( for i in deduplicated/*/*.hr.gz; do j=`echo $i|sed -e 's/[^\/]*\/\(.*\)\.hr\.gz$/\1/'`; hector_server -c WebCorpus.xml -fb filter_unaccented $i accented/$j.hr.gz cssk_top_words.txt accented/$j.log; done )
real    414m2.564s
user    411m59.120s
sys     0m33.950s

cat accented/clanky/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 1512091 12603770        36420958        500635554       598124434       0       0       0       0
cat accented/diskuse/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 2538367 40967072        86284439        1200428898      1475785501      0       0       0       0
cat accented/tvorivost/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 2713952 50685790        111884548       1113912020      1415745963      0       0       0       0
cat accented/*/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 6764410 104256632       234589945       2814976472      3489655898      0       0       0       0

6) language detect + filter

- document level
time ( for i in accented/clanky/*.hr.gz; do j=`echo $i|sed -e 's/[^\/]*\/\(.*\)\.hr\.gz$/\1/'`; hector_server -c WebCorpus.xml -fb filter_language $i czech/$j.hr.gz language_data/top_words cs false czech/$j.log; done )

- paragraph-level
time ( for i in accented/diskuse/*.hr.gz; do j=`echo $i|sed -e 's/[^\/]*\/\(.*\)\.hr\.gz$/\1/'`; hector_server -c WebCorpus.xml -fb filter_language $i czech/$j.hr.gz language_data/top_words cs true czech/$j.log; done )

time ( for i in accented/tvorivost/*.hr.gz; do j=`echo $i|sed -e 's/[^\/]*\/\(.*\)\.hr\.gz$/\1/'`; hector_server -c WebCorpus.xml -fb filter_language $i czech/$j.hr.gz language_data/top_words cs true czech/$j.log; done )

cat czech/clanky/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
cat czech/diskuse/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
cat czech/tvorivost/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
cat czech/*/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin

clanky: M_wordcount[0]: 1352604	11268880	32921318	452585914	541308785	0	0	0	0
diskuse: M_wordcount[0]: 2518864	38215678	81413650	1143073782	1404072625	0	0	0	0
tvorivost: M_wordcount[0]: 2630384	44228416	97558990	983346836	1247586012	0	0	0	0

languages (in words):
clanky:
cs	452585914
sk	3067468
en	501500
other	64281
diskuse:
cs	1143073782
sk	51297087
other	4616883
en	1488875
tvorivost:
cs	983346836
sk	107604069
other	13067099
en	10962822


7) split documents into smaller files

for i in czech/*/*.hr.gz; do j=`echo $i|sed -e 's/[^\/]*\/\(.*\)/\1/'`; hector_server -c WebCorpus.xml -fb split $i split/$j 1048576; done

8) tag all files (using SGE)

for i in split/*/*.hr.gz.*; do j=`echo $i|sed -e 's/^split/tagged/'`; qsub ./tag.sh $i $j; sleep 4; done

9) parse all files
- takes: ???

10) concatenate all files back (just plain cat should do in our case)

11) convert files to vertical format
- word counts (sanity check)

12) create Google n-gram compatible format

-----------------------------------

5) split
Split tokenized.hr file back into files according to servers:
hector_server -c Split.xml -fb split 

Split files, so that we have (upto) 10000 docs in each:
#for i in deduplicated/*.hr.gz; do j=`echo $i|sed -e 's/.*\/\(.*\)\.hr\.gz$/\1/'`; echo hector_server -c WebCorpus.xml -fb split $i $i 10000; done
for i in deduplicated/*.hr.gz; do hector_server -c WebCorpus.xml -fb split $i $i 10000; done
for i in deduplicated/*.hr.gz.*; do j=`echo $i|sed -e 's/\(.*\)\.hr\.gz\.\(.*\)$/\1.hr.\2.gz'`; mv $i $j; done

7) counts for different languages and categories

# count languages
#for i in language/{clanky,diskuse,tvorivost}; do cat $i/*.hr.gz | hector_server -c WebCorpus.xml -fb dump /dev/stdin |grep "^<doc"|sed -e 's/.*lang="\([^"]*\)".*/\1/'|perl -pe 'while (<>) { chomp; foreach $i (split / /) { print "$i\n"; } }' |sort|uniq -c|sort -n; done

8) filter cs + ? languages

- we do not filter currently, just mark, so we skipt this step

time ( for i in language/*/*.hr.gz; do j=`echo $i|sed -e 's/[^\/]*\/\(.*\)\.hr\.gz/\1/'`; hector_server -c WebCorpus.xml -fb filter_language $i language_filtered/$j.hr.gz "cs ?" language_filtered/$j.log; done )
- takes: 1h31m
- word counts:

languge filter:
ls tagged/diskuse/*|grep -v '\.log$'| while read i; do j=`echo $i|sed -e 's/[^\/]*\/\(.*\.hr\.gz.*\)$/\1/'`; hector_server -c webcorpus.xml -fb filter_language $i czech/$j language_data/top_words cs true czech/$j.log; done
ls tagged/{clanky,tvorivost}/*|grep -v '\.log$'| while read i; do j=`echo $i|sed -e 's/[^\/]*\/\(.*\.hr\.gz.*\)$/\1/'`; hector_server -c WebCorpus.xml -fb filter_language $i czech/$j language_data/top_words cs false czech/$j.log; done
