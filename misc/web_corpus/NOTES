1) prepare lists

rm -f data/*/*.list; for i in data/{clanky,diskuse,tvorivost}/*; do find -L $i -type f >$i.list; done

2) tokenize raw texts (+normalize)

time ( for i in data/*/*.list; do j=`echo $i|sed -e 's/[^\/]*\/\(.*\)\.list/\1/'`; hector_server -c WebCorpus.xml -fb tokenize $i tokenized/$j.hr.gz tokenized/$j.log; done )
real    246m47.748s
user    210m41.340s
sys     5m20.950s
- create data dump:
for i in tokenized/*/*.hr.gz; do j=`echo $i|sed -e 's/\.hr\.gz$/.dump/'`; hector_server -c WebCorpus.xml -fb dump $i >$j; done

3) word count for different groups:
cat tokenized/clanky/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 1580493 14261018        41530385        568732717       686604997       0       0       0       0
cat tokenized/diskuse/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 2578840 54527889        114528831       1674143836      2083883744      0       0       0       0
cat tokenized/tvorivost/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 2831249 64050936        150339872       1574181258      2031820433      0       0       0       0
cat tokenized/*/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 6990582 132839843       306399088       3817057811      4802309174      0       0       0       0

4) deduplicate (all at once), split all.hr.gz back into separate files

time ( cat tokenized/*/*.hr.gz |hector_server -c WebCorpus.xml -fb deduplicate_all 6000000000 /dev/stdin deduplicated/all.hr.gz deduplicated/deduplicate.log )
real    290m39.668s
user    289m5.890s
sys     1m8.620s
- word counts (before/after)
M_wc1[0]: 6990582       132839843       306399088       3817057811      4802309174      0       0       0       0
M_wc2[0]: 6819394       116223396       259753763       3255271542      4030066932      0       0       0       0

for i in data/*/*.list; do j=`echo $i|sed -e 's/[^\/]*\/\(.*\)\.list$/\1/'`; echo $j; done|./generate_split.pl >Split.xml
time ( hector_server -c Split.xml -fb split )
real    74m29.675s
user    107m40.950s
sys     0m42.720s

cat deduplicated/clanky/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 1512260 12617177        36461450        501251489       598926543       0       0       0       0
cat deduplicated/diskuse/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 2566027 51551081        107697632       1579254326      1938789786      0       0       0       0
cat deduplicated/tvorivost/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 2741107 52055138        115594681       1174765727      1492350603      0       0       0       0
cat deduplicated/*/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 6819394 116223396       259753763       3255271542      4030066932      0       0       0       0

5) detect paragraphs with more unaccented words

time ( for i in deduplicated/*/*.hr.gz; do j=`echo $i|sed -e 's/[^\/]*\/\(.*\)\.hr\.gz$/\1/'`; hector_server -c WebCorpus.xml -fb filter_unaccented $i accented/$j.hr.gz cssk_top_words.txt accented/$j.log; done )
real    414m2.564s
user    411m59.120s
sys     0m33.950s

cat accented/clanky/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 1512091 12603770        36420958        500635554       598124434       0       0       0       0
cat accented/diskuse/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 2538367 40967072        86284439        1200428898      1475785501      0       0       0       0
cat accented/tvorivost/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 2713952 50685790        111884548       1113912020      1415745963      0       0       0       0
cat accented/*/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 6764410 104256632       234589945       2814976472      3489655898      0       0       0       0

6) language detect + filter

- document level
time ( for i in accented/clanky/*.hr.gz; do j=`echo $i|sed -e 's/[^\/]*\/\(.*\)\.hr\.gz$/\1/'`; hector_server -c WebCorpus.xml -fb filter_language $i czech/$j.hr.gz language_data/top_words cs false czech/$j.log; done )
real    46m28.904s
user    46m3.100s
sys     0m15.010s

- paragraph-level
time ( for i in accented/diskuse/*.hr.gz; do j=`echo $i|sed -e 's/[^\/]*\/\(.*\)\.hr\.gz$/\1/'`; hector_server -c WebCorpus.xml -fb filter_language $i czech/$j.hr.gz language_data/top_words cs true czech/$j.log; done )
real    123m52.773s
user    123m13.340s
sys     0m20.410s

time ( for i in accented/tvorivost/*.hr.gz; do j=`echo $i|sed -e 's/[^\/]*\/\(.*\)\.hr\.gz$/\1/'`; hector_server -c WebCorpus.xml -fb filter_language $i czech/$j.hr.gz language_data/top_words cs true czech/$j.log; done )
real    116m53.580s
user    116m7.860s
sys     0m15.620s

cat czech/clanky/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 1505227 12529047        36185513        496998792       593788404       0       0       0       0
cat czech/diskuse/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 2518865 38214838        81798902        1143025931      1404015135      0       0       0       0
cat czech/tvorivost/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 2629526 44215948        97894796        982272188       1246330942      0       0       0       0
cat czech/*/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 6653618 94959833        215879211       2622296911      3244134481      0       0       0       0

languages (in words):
clanky:
cs	496998792
sk	3068696
en	502645
other	65421
diskuse:
cs	1143025931
sk	51297131
other	4616945
en	1488891
tvorivost:
cs	982272188
sk	107607917
other	13068492
en	10963423
celkem:
cs	2622296911
sk	161973744
other	17750858
en	12954959

7) split documents into smaller files

for i in czech/*/*.hr.gz; do j=`echo $i|sed -e 's/[^\/]*\/\(.*\)/\1/'`; hector_server -c WebCorpus.xml -fb split $i split/$j 1048576; done

8) tag all files (using SGE)

for i in split/*/*.hr.gz.*; do j=`echo $i|sed -e 's/^split/tagged/'`; qsub ./tag.sh $i $j; sleep 4; done

9) parse all files

#ls tagged/*/*.hr.gz.*|grep -v '\.log$'|while read i; do j=`echo $i|sed -e 's/^tagged/parsed/'`; qsub ./parse.sh $i $j; sleep 4; done
ls tagged/*/*.hr.gz.*|grep -v '\.log$'|while read i; do j=`echo $i|sed -e 's/^tagged/parsed/'`; if [ ! -f "$j.log" ]; then qsub ./parse.sh $i $j; sleep 4; fi; done

10) concatenate all files back (just plain cat should do in our case)

11) convert files to vertical format
- word counts (sanity check)

12) create Google n-gram compatible format

- google has:
Total number of tokens:	1,306,807,412,486
Total number of sentences:	 150,727,365,731
Total number of unigrams:	95,998,281
Total number of bigrams:	646,439,858
Total number of trigrams:	1,312,972,925
Total number of fourgrams:	1,396,154,236
Total number of fivegrams:	1,149,361,413
Total number of n-grams:	4,600,926,713

tokens:
sentences:
...

accessoire Annuaire LOEIL	49
