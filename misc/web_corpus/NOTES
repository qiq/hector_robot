1) prepare lists

rm -f data/*/*.list; for i in data/{clanky,diskuse,tvorivost}/*; do find -L $i -type f >$i.list; done

2) tokenize raw texts (+normalize)

time ( for i in data/*/*.list; do j=`echo $i|sed -e 's/[^\/]*\/\(.*\)\.list/\1/'`; hector_server -c WebCorpus.xml -fb tokenize $i tokenized/$j.hr.gz tokenized/$j.log; done )
real    246m47.748s
user    210m41.340s
sys     5m20.950s
- create data dump:
for i in tokenized/*/*.hr.gz; do j=`echo $i|sed -e 's/\.hr\.gz$/.dump/'`; hector_server -c WebCorpus.xml -fb dump $i >$j; done

3) word count for different groups:
cat tokenized/clanky/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
#M_wordcount[0]: 1580493 14261018        41530385        568732717       686604997       0       0       0       0
M_wordcount[0]: 1742463 15524228        44713263        610848438       736784495       0       0       0       0
cat tokenized/diskuse/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
#M_wordcount[0]: 2578840 54527889        114528831       1674143836      2083883744      0       0       0       0
M_wordcount[0]: 2577228 54527889        114528831       1674143836      2083883744      0       0       0       0
cat tokenized/tvorivost/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
#M_wordcount[0]: 2831249 64050936        150339872       1574181258      2031820433      0       0       0       0
M_wordcount[0]: 2837309 64074689        150400043       1575014125      2032861842      0       0       0       0
cat tokenized/*/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
#M_wordcount[0]: 6990582 132839843       306399088       3817057811      4802309174      0       0       0       0
M_wordcount[0]: 7157000 134126806       309642137       3860006399      4853530081      0       0       0       0

4) deduplicate (all at once), split all.hr.gz back into separate files

time ( cat tokenized/*/*.hr.gz |hector_server -c WebCorpus.xml -fb deduplicate_all 6000000000 /dev/stdin deduplicated/all.hr.gz deduplicated/deduplicate.log )
real    290m39.668s
user    289m5.890s
sys     1m8.620s
- word counts (before/after)
#M_wc1[0]: 6990582       132839843       306399088       3817057811      4802309174      0       0       0       0
#M_wc2[0]: 6819394       116223396       259753763       3255271542      4030066932      0       0       0       0
final: M_wc1[0]: 7157000       134462453       310415534       3860016346      4863621189      0       0       0       0
final: M_wc2[0]: 6975869       117402379       262560346       3285087887      4073350543      0       0       0       0

for i in data/*/*.list; do j=`echo $i|sed -e 's/[^\/]*\/\(.*\)\.list$/\1/'`; echo $j; done|./generate_split.pl >Split.xml
time ( hector_server -c Split.xml -fb split )
real    74m29.675s
user    107m40.950s
sys     0m42.720s

cat deduplicated/clanky/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
#M_wordcount[0]: 1512260 12617177        36461450        501251489       598926543       0       0       0       0
M_wordcount[0]: 1663517 13542813        38676184        530924446       633752493       0       0       0       0
cat deduplicated/diskuse/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
#M_wordcount[0]: 2566027 51551081        107697632       1579254326      1938789786      0       0       0       0
M_wordcount[0]: 2566027 51550437        107695796       1579226328      1938755031      0       0       0       0
cat deduplicated/tvorivost/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
#M_wordcount[0]: 2741107 52055138        115594681       1174765727      1492350603      0       0       0       0
M_wordcount[0]: 2745973 52070160        115630722       1175236876      1492932819      0       0       0       0
cat deduplicated/*/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
#M_wordcount[0]: 6819394 116223396       259753763       3255271542      4030066932      0       0       0       0
M_wordcount[0]: 6975517 117163410       262002702       3285387650      4065440343      0       0       0       0

5) detect paragraphs with more unaccented words

time ( for i in deduplicated/*/*.hr.gz; do j=`echo $i|sed -e 's/[^\/]*\/\(.*\)\.hr\.gz$/\1/'`; hector_server -c WebCorpus.xml -fb filter_unaccented $i accented/$j.hr.gz cssk_top_words.txt accented/$j.log; done )
real    414m2.564s
user    411m59.120s
sys     0m33.950s

cat accented/clanky/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 1512091 12603770        36420958        500635554       598124434       0       0       0       0
cat accented/diskuse/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[1]: 2538367 40967072        86284439        1200428898      1475785501      0       0       0       0
cat accented/tvorivost/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 2713952 50685790        111884548       1113912020      1415745963      0       0       0       0
cat accented/*/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 6764410 104256632       234589945       2814976472      3489655898      0       0       0       0

6) language detect + filter

- document level
time ( for i in accented/clanky/*.hr.gz; do j=`echo $i|sed -e 's/[^\/]*\/\(.*\)\.hr\.gz$/\1/'`; hector_server -c WebCorpus.xml -fb filter_language $i czech/$j.hr.gz language_data/top_words cs false czech/$j.log; done )
real    46m28.904s
user    46m3.100s
sys     0m15.010s

- paragraph-level
time ( for i in accented/diskuse/*.hr.gz; do j=`echo $i|sed -e 's/[^\/]*\/\(.*\)\.hr\.gz$/\1/'`; hector_server -c WebCorpus.xml -fb filter_language $i czech/$j.hr.gz language_data/top_words cs true czech/$j.log; done )
real    123m52.773s
user    123m13.340s
sys     0m20.410s

time ( for i in accented/tvorivost/*.hr.gz; do j=`echo $i|sed -e 's/[^\/]*\/\(.*\)\.hr\.gz$/\1/'`; hector_server -c WebCorpus.xml -fb filter_language $i czech/$j.hr.gz language_data/top_words cs true czech/$j.log; done )
real    116m53.580s
user    116m7.860s
sys     0m15.620s

cat czech/clanky/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 1505227 12529047        36185513        496998792       593788404       0       0       0       0
cat czech/diskuse/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 2518865 38214838        81798902        1143025931      1404015135      0       0       0       0
cat czech/tvorivost/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 2629526 44215948        97894796        982272188       1246330942      0       0       0       0
cat czech/*/*.hr.gz | hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 6653618 94959833        215879211       2622296911      3244134481      0       0       0       0

languages (in words):
clanky:
cs	496998792
sk	3068696
en	502645
other	65421
diskuse:
cs	1143025931
sk	51297131
other	4616945
en	1488891
tvorivost:
cs	982272188
sk	107607917
other	13068492
en	10963423
celkem:
cs	2622296911
sk	161973744
other	17750858
en	12954959

7) split documents into smaller files

for i in czech/*/*.hr.gz; do j=`echo $i|sed -e 's/[^\/]*\/\(.*\)/\1/'`; hector_server -c WebCorpus.xml -fb split $i split/$j 1048576; done

8) tag all files (using SGE)

for i in split/*/*.hr.gz.*; do j=`echo $i|sed -e 's/^split/tagged/'`; qsub ./tag.sh $i $j; sleep 4; done

9) parse all files

#ls tagged/*/*.hr.gz.*|grep -v '\.log$'|while read i; do j=`echo $i|sed -e 's/^tagged/parsed/'`; qsub ./parse.sh $i $j; sleep 4; done
ls tagged/*/*.hr.gz.*|grep -v '\.log$'|while read i; do j=`echo $i|sed -e 's/^tagged/parsed/'`; if [ ! -f "$j.log" ]; then qsub ./parse.sh $i $j; sleep 4; fi; done

find parsed/ -type f|grep -v ".log$"|xargs cat |hector_server -c WebCorpus.xml -fb word_count /dev/stdin
M_wordcount[0]: 6653618	94959833	215879211	2622296911	3244134481	3244134481	3244134481	3244134481	3244134481

10) concatenate all files back (just plain cat should do in our case)
find parsed/*/*.log|sed -e 's/\.hr.gz\.[0-9]*\.log$//'|sort -u|while read i; do n=`echo $i.hr.gz.|wc -c`; target=`echo $i|sed -e 's/parsed\/\(.*\)/finished\/\1.hr.gz/'`; ls $i.*|grep -v '\.log'|sort -k 1.$n -n| xargs cat >$target; done

11) create three big final files (and dumps)
for f in clanky diskuse tvorivost; do cat finished/$f/*.hr.gz > finished_$f.hr.gz; done
for f in clanky diskuse tvorivost; do hector_server -c WebCorpus.xml -fb dump finished_${f}.hr.gz > finished_${f}.dump; done

12) convert files to vertical format and prepare for Manatee import
for f in finished_*.vert; do ./vert2manatee.pl <$f|./vert_add_attrs.pl time_map.txt url_map.txt |bzip2 -c >~/manatee_registry/$f.bz; done

13) create permutations

C++ program:
1) mmap data file into memory
2) get sentence boundaries (start -- end, byte offsets)
3) make permutation, write it to the file (Knuth shuffles)
4) go randomly through the file, write parts as necessary

g++ -Wall -ggdb -O3 -o shuffle_sentences shuffle_sentences.cc

for f in clanky diskuse tvorivost; do ./shuffle_sentences finished_$f.vert finished_$f.perm >finished_$f_shuffled.vert; done
gzip -dc finished_clanky.dump.shuffled.gz |./vert2export.pl |bzip2 -c >articles_shuffled.txt.bz2
gzip -dc finished_diskuse.dump.shuffled.gz |./vert2export.pl |bzip2 -c >dicussions_shuffled.txt.bz2
gzip -dc finished_tvorivost.dump.shuffled.gz |./vert2export.pl |bzip2 -c >blogs_shuffled.txt.bz2

14) create Google n-gram compatible data

1) create smaller files
for i in clanky diskuse tvorivost; do hector_server -c WebCorpus.xml -fb split finished_{$f}.hr.gz ngram/$j/finished_${j}.hr.gz 10485760; done

2) extract n-grams from every file
ls ngram/*/*.hr.gz.*|while read i; do qsub ./ngrams.sh $i ngram; sleep 4; fi; done


- google has:
Total number of tokens:	1,306,807,412,486
Total number of sentences:	 150,727,365,731
Total number of unigrams:	95,998,281
Total number of bigrams:	646,439,858
Total number of trigrams:	1,312,972,925
Total number of fourgrams:	1,396,154,236
Total number of fivegrams:	1,149,361,413
Total number of n-grams:	4,600,926,713

tokens:
sentences:
...

accessoire Annuaire LOEIL	49


15) Collocations:

1) create small files (see ngram generation)

2) dump all files (convert to text representation)
FIXME: z tech mensich souboru
for i in sync/*/*.hr.gz.*; do j=`echo $i|sed -e 's/\.hr\.gz\.\(.*\)/.hr.\1/'`; hector_server -c WebCorpus.xml -fb dump $i >$j.dump; done

3) run extraction on all files
for i in sync/*/*.dump; do ./extract_syncs_pdt2.pl <$i >$i.tmp; done

4) merge all files together
ls sync/*/*.dump.tmp | ./merge_syncs.pl

5) create index (word-pos -> offset)
