Module for scanning index and selecting URLs to be downloaded
Module for merging Index with Pages

New wsm: loads resources, loads sites, downloads, save sites...

----------------------
- umet processit relativni Location
	- detekce relativniho URL (://) a resolve
	- otestovat na real datech (robot.sh)
- pridat do fetch/resolver debug o uspesnem vysledku
	- obecne vsechny interakce se servery, sjednotit debug, errors, ...
	- pridat trace, default level bude info(?)

- kazdy modul by mel mit jeden trace event, kde se rika, co se dela

- robots a redirect...
	- kdyz bude robots.txt redirect, tak musis ziskat robots.txt pro jiny
	  wsr (pripadne i dns) a nastav, ze po resolvu se zkopiruji robots info do jineho resource
	- otestovat na nejake domene, melo byt fungovat
- scheme filter: obecne filter!

- ke kazdemu modulu, co zpracovava za resources

testy:
	urlextractor
		- otestovat base

- http://www.searchtools.com/robots/robot-checklist.html

- disable libunbound cache (resp. konfigurace cesty ke konfiguracnimu souboru)
- upravit resourceinfo, aby fungoval pro websiteresource
- items: jen korektne zpracovane resources, ne vsechny -- predelat
- jmena metod na velka pismena (krome getteru/setteru)

- website manager
	- OK nacitat a ukladat websiteresources
	- OK: poustet dns resolution + robots.txt download
	- OK: bude si ukladat websites v hash tabulce, paths v judy array/hash
	- TODO: zpracovavat newlink

- website resource
	- hostname: domenove jmeno 20 + 4 + 16 + 4 + hash + 4 + hash? + 32 lock
	- ip4addr: adresa
	- ip6addr: adresa
	- dnsExpire: kdy vyprsi preklad
	- paths: seznam path
		- name: cele jmeno (je v hash/judy array)
		- updatePeriod: jak casto obnovovat (kvantizovana hodnota, asi 4-255, bude to nejak v minutach 2^n)
			- upravuje se po kazdem stahnuti -- kdyz se zmena udala, tak se snizi, kdyz neudala, tak se o 1 zvysi
		- lastUpdate: kdy se obnovila stranka naposledy (4)
		- lastStatus: jestli byla stranka nalezena ci nikoliv (4)
		- cil presmerovani (optional)
	 - robotsLastUpdate: posledni obnova robots.txt
	 - deniedPaths: seznam zakazanych path (z robots.txt)

- detekce opakujicich se retezcu v URL (staci jeden asi, neco jako /iso/test/iso)
	- to bude specialni filtr, zatim asi v perlu, co bude kontrolovat adresy

- query/response resource
	- getPathInfo: I: hostname, path; O: ip adresa, cksum, error
	- addPath: I: hostname, path; O: OK, already presend
	- pathChange: I: hostname, path, cksum; O: OK
	- list all sites: I: 0; out: [string]
	- list paths: I: hostname; out: [string]

Filter: rozdelit, presunout do hector_core, prepsat modify
filter: umet pracovat i s jednotlivou hlavickou (clear), umet filtrovat hlavicku (jakoze podle jmena, nechat jen specificke)
	* => keep()
- presunout filter do hector_core

- upravit filter: zmenit na filter + modify

- testy pro ruzne moduly
	- (Perl i C++)

- reorder modul
- cistici modul

----
DONE
----

- odstranit curl ze stromu, udelat z toho zvlastni knihovnu, napr. hector_curl
	- googleurl je kratke, to ponechame

- meta refresh -- podpora (asi jen extrakce url)
- url extractor
	- skladani url (base, zaklad z wr)
	- dodelat do wr ResolveUrl, kopie s pomoci base
	- generovani novych wr
- OTESTOVAT: resolver module v C++
  	http://www.catonmat.net/blog/asynchronous-dns-resolution#comments
	- bude pouzivat libunbound -- standardni operace, ale s pouzitim libev
	- vezme host, vrati IP adresu (plus do kdy je platna)
	- multi module
- OTESTOVAT: stahovaci modul
	v C++ pomoci curl (a libevent/libev)
- odstranit UrlParser a UrlComposer
- ve zpracovani robots.txt je potreba nastaveni jmena, to bude parametr
- zmena WebResource - aby umel parsovat a skladat url
- zrusit time pro web resource, bude se to pamatovat ve WSR
- odstranit url-list z webresource, naopak pridat pocet presmerovani
- modul pro zpracovani robots.txt (wr + wsr, vypln ve wsr)
- dodelat testy, pocitat s lokalnim web serverem (v Perlu)
- lokalni dns server (v Perlu) pouzit pro test dns a dns + perl
- test na webresource
- nacitani a ukladani we websitemanager
- dodelat websiteresource, aby pouzival jarray
- pridat detekci jarray do configure.ac
- filter: upravit, aby nevyzadoval WebResource
- resolver module (blokujici, v Perlu)
	- jen obyc prevede jmeno na IP adresu
- CreateWebResource: precist seznam URL a vygenerovat WebResources
	- bylo by pekne, aby slo pridavat URL i za chodu, jenze aby to
	  slo snadno, tak se to musi napsat v C++ (v Perlu to nejde,
	  kvuli zamcenemu objektu, kdyz dojdou URL, musi se modul uspat
	  -- a na to musi byt condition variable. Jenze do Perlu muze
	  jen jedno vlakno naraz.
- stahovaci modul (v Perlu), parametry se jeste uvidi
- url parsing module
	- dostane URL, udela z nej komponenty (pomoci google-url knihovny)
	- ty preda dal
